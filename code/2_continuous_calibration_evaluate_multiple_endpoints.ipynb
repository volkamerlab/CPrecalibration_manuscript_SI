{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Studying and mitigating the effects of data drifts on ML model performance*\n",
    "## Example notebook:  Recalibration strategy for multiple endpoints, evaluate using boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the supporting information to the manuscript entitled: *Studying and mitigating the effects of data drifts on ML model performance at the example of chemical toxicity data*\n",
    "* Authors: A. Morger, M. Garcia de Lomana, U. Norinder, F. Svensson, J. Kirchmair, M. Mathea, A. Volkamer\n",
    "* Last updated: September 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates \n",
    "* How to build and evaluate models for several endpoints at a time: For each endpoint, ACP's are trained without and with updating the calibration set. In this example, the calibration set is even updated twice. \n",
    "* How the predictions could be evaluated using boxplots: For comparison over all endpoints, the balanced validity, balanced efficiency and balanced accuracy are calculated and plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nonconformist.nc import InverseProbabilityErrFunc, NcFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from continuous_calibration import (\n",
    "StratifiedRatioSampler, BalancedStratifiedRatioSampler, CrossValidationSampler, InductiveConformalPredictor, \n",
    "    ContinuousCalibrationAggregatedConformalPredictor, CrossValidator\n",
    ")\n",
    "from continuous_calibration_helper_functions import boxplot_val_eff_acc_quer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that due to the size of the collected ChEMBL datasets containing the precalculated ChemBio descriptors, they were uploaded to [Zenodo](https://zenodo.org/record/5167636). Please download them first and copy the compressed file to the `data` folder to continue with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with all 12 ChEMBL endpoints. \n",
    "data_path = \"../data/chembl_chembio_descriptors.tar.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset defining the splits by publication year into the CP subsets.\n",
    "# Contains information for all 12 endpoints.\n",
    "time_split_threshold_path = \"../data/data_size_chembio_chembl.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHEMBL279', 'CHEMBL220', 'CHEMBL4078', 'CHEMBL5763', 'CHEMBL203', 'CHEMBL206', 'CHEMBL222', 'CHEMBL228', 'CHEMBL230', 'CHEMBL340', 'CHEMBL240', 'CHEMBL2039']\n"
     ]
    }
   ],
   "source": [
    "# Note: Running the calculations for all endpoints takes time. If you only want to test the notebook,\n",
    "# it is recommended to only select a few endpoints (i.e. comment the rest out)\n",
    "endpoint_dict = {\n",
    "    \"CHEMBL279\": [\"VEGFR 2\", \"Vascular endothelial growth factor receptor 2\"],\n",
    "    \"CHEMBL220\": [\"Acetylcholinesterase\"],\n",
    "    \"CHEMBL4078\": [\"Acetylcholinesterase\"],\n",
    "    \"CHEMBL5763\": [\"Cholinesterase\"],\n",
    "    \"CHEMBL203\": [\"EGFR erbB1\", \"Epidermal growth factor receptor erbB1\"],\n",
    "    \"CHEMBL206\": [\"Estrogen receptor alpha\"],\n",
    "    \"CHEMBL222\": [\"Norepinephrine transporter\"],\n",
    "    \"CHEMBL228\": [\"Serotonin transporter\"],\n",
    "    \"CHEMBL230\": [\"Cyclooxygenase-2\"],\n",
    "    \"CHEMBL340\": [\"Cytochrome P450 3A4\"],\n",
    "    \"CHEMBL240\": [\"HERG\"],\n",
    "    \"CHEMBL2039\": [\"Monoamine oxidase B\"],\n",
    "}\n",
    "\n",
    "endpoints = list(endpoint_dict.keys())\n",
    "print(endpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define machine learning parameters.\n",
    "\n",
    "Note that the calculations take time and computational resources. If you just want to try out the notebook, we recommend using the small values provided. To get the same plots as in the manuscript (maybe small deviations due to random splitting and random forest), replace them by the larger values (as provided in the comments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 20 # 500\n",
    "n_folds_acp = 3 # 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define boxplot parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategies to compare\n",
    "strategies = [\"cv_original\", \"original\", \"update1\", \"update2\"]\n",
    "# Evaluation measures to display in boxplot\n",
    "evaluation_measures = [\"validity_bal\", \"efficiency_bal\", \"accuracy_bal\"]\n",
    "significance_level = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_per_endpoint(endpoint_df, endpoint_id):\n",
    "    \"\"\"\n",
    "    Load endpoint dataframe and return arrays holding labels, descriptors and publication years for the data set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    endpoint_df : pd dataframe\n",
    "        Dataframe containing descripor, label, and measured year information per compound.\n",
    "    endpoint_id : string\n",
    "        ChEMBL Id of the endpoint.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X: np array\n",
    "        Descriptors per compound\n",
    "    y: np array\n",
    "        Labels per compoound\n",
    "    years: np array\n",
    "        Publication year per compound\n",
    "    \n",
    "    \"\"\"\n",
    "       \n",
    "    # Drop compound without publication year\n",
    "    endpoint_df.dropna(subset=[\"year\"], inplace=True)\n",
    "    # Define labels for the ML experiment\n",
    "    y = endpoint_df[f\"{endpoint_id}_bioactivity\"].values\n",
    "    # Define feature columns from the ChemBio descriptors, i.e. descelect \n",
    "    # other columns, which are not p-values and chemical descriptors\n",
    "    columns = [\n",
    "            col\n",
    "            for col in endpoint_df.columns\n",
    "            if (not col.startswith(\"Toxicity\"))  # Exclude in vivo labels used for p-values\n",
    "            and (col != f\"{endpoint_id}_bioactivity\")  # Exclude ChEMBL bioactivity labels\n",
    "            and (col != \"SMILES (Canonical)\")  # Exclude canonical smiles\n",
    "            and (col != \"smiles\")  # Exclude smiles\n",
    "            and (col != \"year\")  # Exclude publication year\n",
    "            and (col != \"molecule_chembl_id\")  # Exclude molecule ChEMBL id\n",
    "        ]\n",
    "    # Define descriptors\n",
    "    X = endpoint_df[columns].values\n",
    "    # Define array with publication years\n",
    "    years = endpoint_df[\"year\"].values\n",
    "    return X, y, years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_per_endpoint(X, y, years, splits_df, endpoint_id):\n",
    "    \"\"\"\n",
    "    Split dataset into training, update1, update2 and holdout set based on publication year.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np array\n",
    "        Descriptors per compound\n",
    "    y: np array\n",
    "        Labels per compoound\n",
    "    years: np array\n",
    "        Publication year per compound\n",
    "    splits_df: pd dataframe\n",
    "        Splitting year thresholds\n",
    "    endpoint_id : string\n",
    "        ChEMBL Id of the endpoint.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_train, X_update1, X_update2, X_holdout: np arrays\n",
    "        Descriptors per compoundper split\n",
    "    y_train, y_update1, y_update2, y_holdout: np arrays\n",
    "        Labels per compoound per split\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Collect the year thresholds to split the data\n",
    "    thresholds = splits_df[\"train_thresh\"][endpoint_id], splits_df[\"update1_thresh\"][endpoint_id], splits_df[\"update2_thresh\"][endpoint_id]\n",
    "    \n",
    "    # Get the indices that define how to split the data\n",
    "    mask_train = years <= thresholds[0]\n",
    "    mask_update1 = (years > thresholds[0]) & (years <= thresholds[1])\n",
    "    mask_update2 = (years > thresholds[1]) & (years <= thresholds[2])\n",
    "    mask_holdout = years > thresholds[2]\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, y_train = X[mask_train], y[mask_train]\n",
    "    X_update1, y_update1 = X[mask_update1], y[mask_update1]\n",
    "    X_update2, y_update2 = X[mask_update2], y[mask_update2]\n",
    "    X_holdout, y_holdout = X[mask_holdout], y[mask_holdout]\n",
    "    \n",
    "    return X_train, X_update1, X_update2, X_holdout, y_train, y_update1, y_update2, y_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rf_acp(n_rf_trees, train_split_acp):\n",
    "    \"\"\"\n",
    "    Prepare and return an aggregated conformal predictor (ACP).\n",
    "    Default parameteres: random forest classifier, inverse probability error function, mondrian condition.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_rf_trees: int\n",
    "        Number of trees in the forest.\n",
    "    train_split_acp: int\n",
    "        Training data split for ACP (default: 70% proper training and 30% calibration set)\n",
    "     \n",
    "    Returns\n",
    "    -------\n",
    "    acp: continuous_calibration ACP\n",
    "        Initialized ACP object\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define ML classifier (random forest)\n",
    "    clf = RandomForestClassifier(n_estimators=n_rf_trees)\n",
    "    # Define error function for conformal prediction (inverse probability error function)\n",
    "    error_function = InverseProbabilityErrFunc()\n",
    "    # Define nonconformity function object\n",
    "    nc = NcFactory.create_nc(clf, err_func=error_function, normalizer_model=None)\n",
    "    # Build inductive conformal predictor (mondrian condition)\n",
    "    print((lambda instance: instance[1]))\n",
    "    icp = InductiveConformalPredictor(nc_function=nc, condition=(lambda instance: instance[1]), \n",
    "                                      smoothing=False)\n",
    "    # Define how to split training data into proper training and calibration set\n",
    "    # (random, stratified, 70% proper training, 30% calibration set)\n",
    "    ratio_sampler = StratifiedRatioSampler(n_folds=train_split_acp)\n",
    "    # Build aggregated conformal predictor, aggregate predictions by median\n",
    "    acp = ContinuousCalibrationAggregatedConformalPredictor(\n",
    "        predictor=icp, sampler=ratio_sampler, aggregation_func=np.median)\n",
    "    \n",
    "    return acp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpack prepared ChEMBL datasets \n",
    "\n",
    "Containing the \n",
    "* ChemBio descriptors, \n",
    "* labels (activity), \n",
    "* and publication year\n",
    "\n",
    "Due to the size of the datasets containing the chembio descriptors, they were uploaded to [zenodo](https://zenodo.org/record/5167636) as `tar.bz2` files and need to be unpacked first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack file\n",
    "tar = tarfile.open(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is collected in the `files` dict, containing as keys the ChEMBL endpoint name and as value the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary containing the input descriptor files\n",
    "endpoint_data_file_dict = {}\n",
    "# Unpack the individual files\n",
    "for i, name in zip(tar, tar.getnames()):\n",
    "    file = tar.extractfile(i)\n",
    "    # Extraction also returns the empty folder (where the\n",
    "    # Twelve datasets were stored), which will be ignored\n",
    "    if file:  \n",
    "        n = name.split(\"/\")[1].split('_')[0]  # Get ChEMBL endpoint name from filename     \n",
    "        df = pd.read_csv(file)  # Read df\n",
    "        endpoint_data_file_dict[n] = df  # Store in dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataframe defining time-split thresholds per endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_thresh</th>\n",
       "      <th>update1_thresh</th>\n",
       "      <th>update2_thresh</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chembl_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CHEMBL220</th>\n",
       "      <td>2014</td>\n",
       "      <td>2016</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHEMBL4078</th>\n",
       "      <td>2014</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHEMBL5763</th>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHEMBL203</th>\n",
       "      <td>2012</td>\n",
       "      <td>2014</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHEMBL206</th>\n",
       "      <td>2006</td>\n",
       "      <td>2012</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHEMBL279</th>\n",
       "      <td>2010</td>\n",
       "      <td>2013</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHEMBL230</th>\n",
       "      <td>2010</td>\n",
       "      <td>2013</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHEMBL340</th>\n",
       "      <td>2012</td>\n",
       "      <td>2014</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHEMBL240</th>\n",
       "      <td>2012</td>\n",
       "      <td>2014</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHEMBL2039</th>\n",
       "      <td>2014</td>\n",
       "      <td>2015</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHEMBL222</th>\n",
       "      <td>2009</td>\n",
       "      <td>2011</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHEMBL228</th>\n",
       "      <td>2009</td>\n",
       "      <td>2011</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            train_thresh  update1_thresh  update2_thresh\n",
       "chembl_id                                               \n",
       "CHEMBL220           2014            2016            2017\n",
       "CHEMBL4078          2014            2015            2016\n",
       "CHEMBL5763          2015            2016            2017\n",
       "CHEMBL203           2012            2014            2016\n",
       "CHEMBL206           2006            2012            2016\n",
       "CHEMBL279           2010            2013            2014\n",
       "CHEMBL230           2010            2013            2015\n",
       "CHEMBL340           2012            2014            2015\n",
       "CHEMBL240           2012            2014            2016\n",
       "CHEMBL2039          2014            2015            2017\n",
       "CHEMBL222           2009            2011            2015\n",
       "CHEMBL228           2009            2011            2014"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_df = pd.read_csv(time_split_threshold_path, index_col=0, \n",
    "                        usecols=[\"chembl_id\", \"train_thresh\", \"update1_thresh\", \"update2_thresh\"])\n",
    "splits_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformal prediction recalibration strategy for multiple endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment illustrating the recalibration strategy for multiple endpoints includes the following steps:\n",
    "* Prepare data\n",
    "* Define ACP and cross-validator objects\n",
    "* Cross-validate using `original` calibration set, predict holdout set\n",
    "* Recalibrate using update1 calibration set, predict holdout set\n",
    "* Recalibrate using update2 calibration set, predict holdout set\n",
    "* Store dataframes with evaluation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running recalibration experiment for CHEMBL279 endpoint\n",
      "<function prepare_rf_acp.<locals>.<lambda> at 0x7f82a79285e0>\n",
      "Running recalibration experiment for CHEMBL220 endpoint\n",
      "<function prepare_rf_acp.<locals>.<lambda> at 0x7f82a9b2ee50>\n",
      "Running recalibration experiment for CHEMBL4078 endpoint\n",
      "<function prepare_rf_acp.<locals>.<lambda> at 0x7f82a936d8b0>\n",
      "Running recalibration experiment for CHEMBL5763 endpoint\n",
      "<function prepare_rf_acp.<locals>.<lambda> at 0x7f82a79285e0>\n",
      "Running recalibration experiment for CHEMBL203 endpoint\n",
      "<function prepare_rf_acp.<locals>.<lambda> at 0x7f82a936d8b0>\n",
      "Running recalibration experiment for CHEMBL206 endpoint\n",
      "<function prepare_rf_acp.<locals>.<lambda> at 0x7f82a93b30d0>\n",
      "Running recalibration experiment for CHEMBL222 endpoint\n",
      "<function prepare_rf_acp.<locals>.<lambda> at 0x7f82a93fc430>\n",
      "Running recalibration experiment for CHEMBL228 endpoint\n",
      "<function prepare_rf_acp.<locals>.<lambda> at 0x7f82a79285e0>\n"
     ]
    }
   ],
   "source": [
    "evaluation_dfs = {\"cv_original\": [], 'original': [], \n",
    "                  'update1': [], 'update2': []}\n",
    "for endpoint in endpoints:\n",
    "    \n",
    "    print(f\"Running recalibration experiment for {endpoint} endpoint\")\n",
    "    \n",
    "    # Prepare data\n",
    "    endpoint_data_df = endpoint_data_file_dict[endpoint]\n",
    "    X, y, years = load_data_per_endpoint(endpoint_data_df, endpoint)\n",
    "    # Get the descriptors and labels for the four splits\n",
    "    X_train, X_update1, X_update2, X_holdout,\\\n",
    "    y_train, y_update1, y_update2, y_holdout = split_data_per_endpoint(\n",
    "        X, y, years, splits_df, endpoint\n",
    "    )\n",
    "    \n",
    "    # Define the RF-based ACP and cross-validator\n",
    "    acp = prepare_rf_acp(n_trees, n_folds_acp)\n",
    "    cross_validator = CrossValidator(predictor=acp, cv_splitter=CrossValidationSampler())\n",
    "    # Cross-validate using 'original' calibration set\n",
    "    cross_validator.cross_validate(\n",
    "        X_train=X_train, y_train=y_train, X_test=X_holdout, y_test=y_holdout,\n",
    "        steps=10, endpoint=endpoint\n",
    "    )\n",
    "    \n",
    "    # Recalibrate using update1 calibration set within above crossvalidation loop\n",
    "    cross_validator.cross_validate_calibrate_update(\n",
    "        X_update=X_update1, y_update=y_update1, X_test=X_holdout, y_test=y_holdout, \n",
    "        steps=10, endpoint=endpoint\n",
    "    )\n",
    "    \n",
    "    # Recalibrate using update2 calibration set within above crossvalidation loop\n",
    "    cross_validator.cross_validate_calibrate_update(\n",
    "        X_update=X_update2, y_update=y_update2, X_test=X_holdout, y_test=y_holdout, \n",
    "        steps=10, endpoint=endpoint\n",
    "    )\n",
    "    \n",
    "    # Store dataframes with evaluation values\n",
    "    evaluation_dfs[\"original\"].append(cross_validator.averaged_evaluation_df_pred_test)\n",
    "    evaluation_dfs[\"cv_original\"].append(cross_validator.averaged_evaluation_df_cv)\n",
    "    evaluation_dfs[\"update1\"].append(cross_validator.averaged_evaluation_df_cal_update_1)\n",
    "    evaluation_dfs[\"update2\"].append(cross_validator.averaged_evaluation_df_cal_update_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots to compare different strategies for multiple endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the results over all endpoints, boxplots can be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = boxplot_val_eff_acc_quer(\n",
    "    evaluation_dfs=evaluation_dfs,\n",
    "    measures=evaluation_measures,\n",
    "    significance_level=significance_level,\n",
    "    map_labels=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
